{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KrS8T2HXnLCV"
      },
      "source": [
        "\n",
        "# Baseline Model Implementation\n",
        "# GCN Implementation\n",
        "# Training & Test all 6 datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "gPYAemkfnURB",
        "outputId": "07fe453f-3b2e-45f1-8033-c1dc77b38ba5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in links: https://data.pyg.org/whl/torch-2.3.0+cu121.html\n",
            "Collecting pyg_lib\n",
            "  Downloading https://data.pyg.org/whl/torch-2.3.0%2Bcu121/pyg_lib-0.4.0%2Bpt23cu121-cp310-cp310-linux_x86_64.whl (2.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m21.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: torch_geometric in /usr/local/lib/python3.10/dist-packages (2.6.1)\n",
            "Requirement already satisfied: torch_scatter in /usr/local/lib/python3.10/dist-packages (2.1.2+pt20cu118)\n",
            "Requirement already satisfied: torch_sparse in /usr/local/lib/python3.10/dist-packages (0.6.18+pt20cu118)\n",
            "Requirement already satisfied: torch_cluster in /usr/local/lib/python3.10/dist-packages (1.6.3+pt20cu118)\n",
            "Requirement already satisfied: torch_spline_conv in /usr/local/lib/python3.10/dist-packages (1.2.2+pt20cu118)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (3.11.9)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (2024.10.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (3.1.4)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (1.26.4)\n",
            "Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (5.9.5)\n",
            "Requirement already satisfied: pyparsing in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (3.2.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (2.28.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (4.65.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from torch_sparse) (1.13.1)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch_geometric) (2.4.4)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch_geometric) (1.3.1)\n",
            "Requirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch_geometric) (4.0.3)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch_geometric) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch_geometric) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch_geometric) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch_geometric) (0.2.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch_geometric) (1.18.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch_geometric) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torch_geometric) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torch_geometric) (3.10)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torch_geometric) (1.26.20)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torch_geometric) (2024.8.30)\n",
            "Requirement already satisfied: typing-extensions>=4.1.0 in /usr/local/lib/python3.10/dist-packages (from multidict<7.0,>=4.5->aiohttp->torch_geometric) (4.12.2)\n",
            "Installing collected packages: pyg_lib\n",
            "Successfully installed pyg_lib-0.4.0+pt23cu121\n"
          ]
        }
      ],
      "source": [
        "!pip install pyg_lib torch_geometric torch_scatter torch_sparse torch_cluster torch_spline_conv -f https://data.pyg.org/whl/torch-2.3.0+cu121.html"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "rSrDUPAnVElT"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch_geometric.nn import GCNConv\n",
        "\n",
        "# Updated GCN Model to accept variable number of layers\n",
        "\n",
        "\n",
        "class GCN(torch.nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, output_dim, num_layers):\n",
        "        super(GCN, self).__init__()\n",
        "        self.convs = torch.nn.ModuleList()\n",
        "        self.convs.append(GCNConv(input_dim, hidden_dim))\n",
        "        for _ in range(num_layers - 2):\n",
        "            self.convs.append(GCNConv(hidden_dim, hidden_dim))\n",
        "        self.convs.append(GCNConv(hidden_dim, output_dim))\n",
        "\n",
        "    def forward(self, x, edge_index):  # Add edge_index as an argument\n",
        "        for i, conv in enumerate(self.convs):\n",
        "            x = conv(x, edge_index)  # Pass edge_index to the conv layers\n",
        "            if i < len(self.convs) - 1:\n",
        "                x = F.relu(x)\n",
        "                x = F.dropout(x, training=self.training)\n",
        "        return F.log_softmax(x, dim=1)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H9QpLwEtqa1J"
      },
      "source": [
        "# Loading Dataset & Generate Masks & Helper Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IDtlSU56qf2C"
      },
      "outputs": [],
      "source": [
        "def load_dataset(dataset_name):\n",
        "    if dataset_name in ['Cora', 'PubMed', 'CiteSeer']:\n",
        "        dataset = Planetoid(root=f'./data/{dataset_name}', name=dataset_name)\n",
        "    elif dataset_name in ['Chameleon', 'Squirrel']:\n",
        "        dataset = WikipediaNetwork(root=f'./data/{dataset_name}', name=dataset_name)\n",
        "    elif dataset_name in ['Texas']:\n",
        "        dataset = WebKB(root=f'./data/{dataset_name}', name=dataset_name)\n",
        "    else:\n",
        "        raise ValueError(\"Dataset not recognized!\")\n",
        "    return dataset\n",
        "\n",
        "def generate_masks(data, train_ratio=0.6, val_ratio=0.2):\n",
        "    num_nodes = data.y.size(0)\n",
        "    indices = torch.randperm(num_nodes)\n",
        "\n",
        "    train_size = int(train_ratio * num_nodes)\n",
        "    val_size = int(val_ratio * num_nodes)\n",
        "\n",
        "    data.train_mask = torch.zeros(num_nodes, dtype=torch.bool)\n",
        "    data.val_mask = torch.zeros(num_nodes, dtype=torch.bool)\n",
        "    data.test_mask = torch.zeros(num_nodes, dtype=torch.bool)\n",
        "\n",
        "    data.train_mask[indices[:train_size]] = True\n",
        "    data.val_mask[indices[train_size:train_size + val_size]] = True\n",
        "    data.test_mask[indices[train_size + val_size:]] = True\n",
        "\n",
        "    return data\n",
        "\n",
        "def train_model(model, data, optimizer, criterion):\n",
        "    model.train()\n",
        "    optimizer.zero_grad()\n",
        "    out = model(data.x, data.edge_index)  # Forward pass\n",
        "    loss = criterion(out[data.train_mask], data.y[data.train_mask].long())  # Loss calculation\n",
        "    loss.backward()  # Backpropagation\n",
        "    optimizer.step()  # Optimizer step\n",
        "    return loss.item()\n",
        "\n",
        "@torch.no_grad()\n",
        "def evaluate_model(model, data, mask):\n",
        "    model.eval()\n",
        "    out = model(data.x, data.edge_index)  # Forward pass\n",
        "    pred = out.argmax(dim=1)  # Predicted class\n",
        "    correct = pred[mask] == data.y[mask]  # Compare predictions with ground truth\n",
        "    acc = int(correct.sum()) / int(mask.sum())  # Calculate accuracy\n",
        "    return acc\n",
        "\n",
        "def fix_masks(data):\n",
        "    # Ensure masks are one-dimensional Boolean tensors\n",
        "    if len(data.train_mask.shape) > 1:\n",
        "        data.train_mask = data.train_mask[:, 0].bool()  # Take the first column if multi-dimensional\n",
        "    if len(data.val_mask.shape) > 1:\n",
        "        data.val_mask = data.val_mask[:, 0].bool()\n",
        "    if len(data.test_mask.shape) > 1:\n",
        "        data.test_mask = data.test_mask[:, 0].bool()\n",
        "    return data\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TnFDLkustqLo"
      },
      "source": [
        "#Training function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IPrNv42btv-h"
      },
      "outputs": [],
      "source": [
        "# In train_single_dataset function:\n",
        "\n",
        "def train_single_dataset(dataset_name):\n",
        "    print(f\"\\n--- Training on {dataset_name} ---\\n\")\n",
        "    dataset = load_dataset(dataset_name)\n",
        "    data = dataset[0]  # Access the single graph data\n",
        "\n",
        "    # Fix masks and labels if needed\n",
        "    data = fix_masks(data)\n",
        "    if len(data.y.shape) > 1:  # If labels are one-hot encoded\n",
        "        data.y = data.y.argmax(dim=1)\n",
        "\n",
        "    # Debug shapes and classes\n",
        "    print(f\"data.y shape: {data.y.shape}, unique labels: {data.y.unique()}\")\n",
        "    print(f\"train_mask shape: {data.train_mask.shape}, sum: {data.train_mask.sum()}\")\n",
        "    print(f\"val_mask shape: {data.val_mask.shape}, sum: {data.val_mask.sum()}\")\n",
        "    print(f\"test_mask shape: {data.test_mask.shape}, sum: {data.test_mask.sum()}\")\n",
        "\n",
        "    # Initialize model\n",
        "    model = GCN(\n",
        "        input_dim=dataset.num_node_features,\n",
        "        hidden_dim=16,\n",
        "        output_dim=dataset.num_classes,\n",
        "        num_layers=2  # Add num_layers argument here. You may need to adjust this value\n",
        "    )\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)\n",
        "    criterion = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "    # Training loop\n",
        "    best_val_acc = 0\n",
        "    for epoch in range(100):  # Train for 100 epochs\n",
        "        train_loss = train_model(model, data, optimizer, criterion)\n",
        "        val_acc = evaluate_model(model, data, data.val_mask)\n",
        "        test_acc = evaluate_model(model, data, data.test_mask)\n",
        "\n",
        "        if val_acc > best_val_acc:\n",
        "            best_val_acc = val_acc\n",
        "            best_test_acc = test_acc\n",
        "\n",
        "        if epoch % 10 == 0:  # Print every 10 epochs\n",
        "            print(f\"Epoch: {epoch+1:03d}, Loss: {train_loss:.4f}, Val Acc: {val_acc:.4f}, Test Acc: {test_acc:.4f}\")\n",
        "\n",
        "    print(f\"\\nBest Validation Accuracy on {dataset_name}: {best_val_acc:.4f}\")\n",
        "    print(f\"Test Accuracy at Best Validation: {best_test_acc:.4f}\")\n",
        "    return best_val_acc, best_test_acc"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QBGNPp6IvG32"
      },
      "source": [
        "#Train Cora Dataset (Homophilic)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "jyB-CY1cvKPi",
        "outputId": "2763a595-b803-4cc4-f73b-a4ee520db854"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Training on Cora ---\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.x\n",
            "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.tx\n",
            "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.allx\n",
            "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.y\n",
            "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.ty\n",
            "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.ally\n",
            "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.graph\n",
            "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.test.index\n",
            "Processing...\n",
            "Done!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "data.y shape: torch.Size([2708]), unique labels: tensor([0, 1, 2, 3, 4, 5, 6])\n",
            "train_mask shape: torch.Size([2708]), sum: 140\n",
            "val_mask shape: torch.Size([2708]), sum: 500\n",
            "test_mask shape: torch.Size([2708]), sum: 1000\n",
            "Epoch: 001, Loss: 1.9462, Val Acc: 0.3180, Test Acc: 0.3130\n",
            "Epoch: 011, Loss: 0.7826, Val Acc: 0.7320, Test Acc: 0.7590\n",
            "Epoch: 021, Loss: 0.2679, Val Acc: 0.7620, Test Acc: 0.7910\n",
            "Epoch: 031, Loss: 0.0868, Val Acc: 0.7540, Test Acc: 0.7830\n",
            "Epoch: 041, Loss: 0.0592, Val Acc: 0.7620, Test Acc: 0.7840\n",
            "Epoch: 051, Loss: 0.0684, Val Acc: 0.7660, Test Acc: 0.7860\n",
            "Epoch: 061, Loss: 0.0509, Val Acc: 0.7720, Test Acc: 0.7830\n",
            "Epoch: 071, Loss: 0.0282, Val Acc: 0.7760, Test Acc: 0.7930\n",
            "Epoch: 081, Loss: 0.0613, Val Acc: 0.7700, Test Acc: 0.7860\n",
            "Epoch: 091, Loss: 0.0476, Val Acc: 0.7660, Test Acc: 0.7930\n",
            "\n",
            "Best Validation Accuracy on Cora: 0.7780\n",
            "Test Accuracy at Best Validation: 0.7930\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.778, 0.793)"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ],
      "source": [
        "train_single_dataset('Cora')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gAhU-YKav4_T"
      },
      "source": [
        "#Train PubMed Dataset (Homophilic)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "DgmA5bA2v7WN",
        "outputId": "da7b9539-e7e5-4f65-81c1-ac60f925d079"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Training on PubMed ---\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.pubmed.x\n",
            "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.pubmed.tx\n",
            "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.pubmed.allx\n",
            "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.pubmed.y\n",
            "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.pubmed.ty\n",
            "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.pubmed.ally\n",
            "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.pubmed.graph\n",
            "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.pubmed.test.index\n",
            "Processing...\n",
            "Done!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "data.y shape: torch.Size([19717]), unique labels: tensor([0, 1, 2])\n",
            "train_mask shape: torch.Size([19717]), sum: 60\n",
            "val_mask shape: torch.Size([19717]), sum: 500\n",
            "test_mask shape: torch.Size([19717]), sum: 1000\n",
            "Epoch: 001, Loss: 1.0981, Val Acc: 0.5660, Test Acc: 0.5540\n",
            "Epoch: 011, Loss: 0.9212, Val Acc: 0.7660, Test Acc: 0.7240\n",
            "Epoch: 021, Loss: 0.6956, Val Acc: 0.7540, Test Acc: 0.7340\n",
            "Epoch: 031, Loss: 0.5082, Val Acc: 0.7700, Test Acc: 0.7600\n",
            "Epoch: 041, Loss: 0.3548, Val Acc: 0.7780, Test Acc: 0.7680\n",
            "Epoch: 051, Loss: 0.2649, Val Acc: 0.7720, Test Acc: 0.7720\n",
            "Epoch: 061, Loss: 0.2142, Val Acc: 0.7780, Test Acc: 0.7750\n",
            "Epoch: 071, Loss: 0.1645, Val Acc: 0.7700, Test Acc: 0.7740\n",
            "Epoch: 081, Loss: 0.1637, Val Acc: 0.7840, Test Acc: 0.7790\n",
            "Epoch: 091, Loss: 0.1050, Val Acc: 0.7840, Test Acc: 0.7850\n",
            "\n",
            "Best Validation Accuracy on PubMed: 0.7860\n",
            "Test Accuracy at Best Validation: 0.7770\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.786, 0.777)"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ],
      "source": [
        "train_single_dataset('PubMed')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CCdHI5KJw7s-"
      },
      "source": [
        "#Train Citeseer Dataset (Homophilic)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "KrBwJaF0xSKx",
        "outputId": "f2cedc25-4dfc-440d-fc95-76a1e2ecae58"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Training on CiteSeer ---\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.citeseer.x\n",
            "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.citeseer.tx\n",
            "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.citeseer.allx\n",
            "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.citeseer.y\n",
            "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.citeseer.ty\n",
            "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.citeseer.ally\n",
            "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.citeseer.graph\n",
            "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.citeseer.test.index\n",
            "Processing...\n",
            "Done!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "data.y shape: torch.Size([3327]), unique labels: tensor([0, 1, 2, 3, 4, 5])\n",
            "train_mask shape: torch.Size([3327]), sum: 120\n",
            "val_mask shape: torch.Size([3327]), sum: 500\n",
            "test_mask shape: torch.Size([3327]), sum: 1000\n",
            "Epoch: 001, Loss: 1.8023, Val Acc: 0.4340, Test Acc: 0.4310\n",
            "Epoch: 011, Loss: 0.3983, Val Acc: 0.6760, Test Acc: 0.6720\n",
            "Epoch: 021, Loss: 0.1137, Val Acc: 0.6720, Test Acc: 0.6670\n",
            "Epoch: 031, Loss: 0.0425, Val Acc: 0.6620, Test Acc: 0.6650\n",
            "Epoch: 041, Loss: 0.0407, Val Acc: 0.6680, Test Acc: 0.6760\n",
            "Epoch: 051, Loss: 0.0458, Val Acc: 0.6860, Test Acc: 0.6820\n",
            "Epoch: 061, Loss: 0.0333, Val Acc: 0.6680, Test Acc: 0.6610\n",
            "Epoch: 071, Loss: 0.0448, Val Acc: 0.6780, Test Acc: 0.6670\n",
            "Epoch: 081, Loss: 0.0358, Val Acc: 0.6700, Test Acc: 0.6650\n",
            "Epoch: 091, Loss: 0.0330, Val Acc: 0.6860, Test Acc: 0.6870\n",
            "\n",
            "Best Validation Accuracy on CiteSeer: 0.6860\n",
            "Test Accuracy at Best Validation: 0.6840\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.686, 0.684)"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ],
      "source": [
        "train_single_dataset('CiteSeer')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3XmWiDmtxhmv"
      },
      "source": [
        "#Train Squirrel Dataset (Heterophilic)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "94caqe0kxjTr",
        "outputId": "9b8ce62e-ef15-4618-fe68-327fcb9fd36a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Training on Squirrel ---\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading https://raw.githubusercontent.com/graphdml-uiuc-jlu/geom-gcn/f1fc0d14b3b019c562737240d06ec83b07d16a8f/new_data/squirrel/out1_node_feature_label.txt\n",
            "Downloading https://raw.githubusercontent.com/graphdml-uiuc-jlu/geom-gcn/f1fc0d14b3b019c562737240d06ec83b07d16a8f/new_data/squirrel/out1_graph_edges.txt\n",
            "Downloading https://raw.githubusercontent.com/graphdml-uiuc-jlu/geom-gcn/f1fc0d14b3b019c562737240d06ec83b07d16a8f/splits/squirrel_split_0.6_0.2_0.npz\n",
            "Downloading https://raw.githubusercontent.com/graphdml-uiuc-jlu/geom-gcn/f1fc0d14b3b019c562737240d06ec83b07d16a8f/splits/squirrel_split_0.6_0.2_1.npz\n",
            "Downloading https://raw.githubusercontent.com/graphdml-uiuc-jlu/geom-gcn/f1fc0d14b3b019c562737240d06ec83b07d16a8f/splits/squirrel_split_0.6_0.2_2.npz\n",
            "Downloading https://raw.githubusercontent.com/graphdml-uiuc-jlu/geom-gcn/f1fc0d14b3b019c562737240d06ec83b07d16a8f/splits/squirrel_split_0.6_0.2_3.npz\n",
            "Downloading https://raw.githubusercontent.com/graphdml-uiuc-jlu/geom-gcn/f1fc0d14b3b019c562737240d06ec83b07d16a8f/splits/squirrel_split_0.6_0.2_4.npz\n",
            "Downloading https://raw.githubusercontent.com/graphdml-uiuc-jlu/geom-gcn/f1fc0d14b3b019c562737240d06ec83b07d16a8f/splits/squirrel_split_0.6_0.2_5.npz\n",
            "Downloading https://raw.githubusercontent.com/graphdml-uiuc-jlu/geom-gcn/f1fc0d14b3b019c562737240d06ec83b07d16a8f/splits/squirrel_split_0.6_0.2_6.npz\n",
            "Downloading https://raw.githubusercontent.com/graphdml-uiuc-jlu/geom-gcn/f1fc0d14b3b019c562737240d06ec83b07d16a8f/splits/squirrel_split_0.6_0.2_7.npz\n",
            "Downloading https://raw.githubusercontent.com/graphdml-uiuc-jlu/geom-gcn/f1fc0d14b3b019c562737240d06ec83b07d16a8f/splits/squirrel_split_0.6_0.2_8.npz\n",
            "Downloading https://raw.githubusercontent.com/graphdml-uiuc-jlu/geom-gcn/f1fc0d14b3b019c562737240d06ec83b07d16a8f/splits/squirrel_split_0.6_0.2_9.npz\n",
            "Processing...\n",
            "Done!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "data.y shape: torch.Size([5201]), unique labels: tensor([0, 1, 2, 3, 4])\n",
            "train_mask shape: torch.Size([5201]), sum: 2496\n",
            "val_mask shape: torch.Size([5201]), sum: 1664\n",
            "test_mask shape: torch.Size([5201]), sum: 1041\n",
            "Epoch: 001, Loss: 1.6361, Val Acc: 0.2326, Test Acc: 0.2469\n",
            "Epoch: 011, Loss: 1.4804, Val Acc: 0.2386, Test Acc: 0.2373\n",
            "Epoch: 021, Loss: 1.3431, Val Acc: 0.2662, Test Acc: 0.2690\n",
            "Epoch: 031, Loss: 1.2305, Val Acc: 0.2614, Test Acc: 0.2613\n",
            "Epoch: 041, Loss: 1.1466, Val Acc: 0.2488, Test Acc: 0.2517\n",
            "Epoch: 051, Loss: 1.0875, Val Acc: 0.2458, Test Acc: 0.2402\n",
            "Epoch: 061, Loss: 1.0256, Val Acc: 0.2398, Test Acc: 0.2488\n",
            "Epoch: 071, Loss: 0.9975, Val Acc: 0.2272, Test Acc: 0.2296\n",
            "Epoch: 081, Loss: 0.9834, Val Acc: 0.2242, Test Acc: 0.2344\n",
            "Epoch: 091, Loss: 0.9560, Val Acc: 0.2296, Test Acc: 0.2334\n",
            "\n",
            "Best Validation Accuracy on Squirrel: 0.2668\n",
            "Test Accuracy at Best Validation: 0.2767\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.2668269230769231, 0.276657060518732)"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ],
      "source": [
        "train_single_dataset('Squirrel')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4KgFd_3KcgX3"
      },
      "source": [
        "#Train Chameleon Dataset (Heterophilic)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "bpolgvf2ct9i",
        "outputId": "8806f418-7296-4f89-944b-6fbca1c22653"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Training on Chameleon ---\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading https://raw.githubusercontent.com/graphdml-uiuc-jlu/geom-gcn/f1fc0d14b3b019c562737240d06ec83b07d16a8f/new_data/chameleon/out1_node_feature_label.txt\n",
            "Downloading https://raw.githubusercontent.com/graphdml-uiuc-jlu/geom-gcn/f1fc0d14b3b019c562737240d06ec83b07d16a8f/new_data/chameleon/out1_graph_edges.txt\n",
            "Downloading https://raw.githubusercontent.com/graphdml-uiuc-jlu/geom-gcn/f1fc0d14b3b019c562737240d06ec83b07d16a8f/splits/chameleon_split_0.6_0.2_0.npz\n",
            "Downloading https://raw.githubusercontent.com/graphdml-uiuc-jlu/geom-gcn/f1fc0d14b3b019c562737240d06ec83b07d16a8f/splits/chameleon_split_0.6_0.2_1.npz\n",
            "Downloading https://raw.githubusercontent.com/graphdml-uiuc-jlu/geom-gcn/f1fc0d14b3b019c562737240d06ec83b07d16a8f/splits/chameleon_split_0.6_0.2_2.npz\n",
            "Downloading https://raw.githubusercontent.com/graphdml-uiuc-jlu/geom-gcn/f1fc0d14b3b019c562737240d06ec83b07d16a8f/splits/chameleon_split_0.6_0.2_3.npz\n",
            "Downloading https://raw.githubusercontent.com/graphdml-uiuc-jlu/geom-gcn/f1fc0d14b3b019c562737240d06ec83b07d16a8f/splits/chameleon_split_0.6_0.2_4.npz\n",
            "Downloading https://raw.githubusercontent.com/graphdml-uiuc-jlu/geom-gcn/f1fc0d14b3b019c562737240d06ec83b07d16a8f/splits/chameleon_split_0.6_0.2_5.npz\n",
            "Downloading https://raw.githubusercontent.com/graphdml-uiuc-jlu/geom-gcn/f1fc0d14b3b019c562737240d06ec83b07d16a8f/splits/chameleon_split_0.6_0.2_6.npz\n",
            "Downloading https://raw.githubusercontent.com/graphdml-uiuc-jlu/geom-gcn/f1fc0d14b3b019c562737240d06ec83b07d16a8f/splits/chameleon_split_0.6_0.2_7.npz\n",
            "Downloading https://raw.githubusercontent.com/graphdml-uiuc-jlu/geom-gcn/f1fc0d14b3b019c562737240d06ec83b07d16a8f/splits/chameleon_split_0.6_0.2_8.npz\n",
            "Downloading https://raw.githubusercontent.com/graphdml-uiuc-jlu/geom-gcn/f1fc0d14b3b019c562737240d06ec83b07d16a8f/splits/chameleon_split_0.6_0.2_9.npz\n",
            "Processing...\n",
            "Done!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "data.y shape: torch.Size([2277]), unique labels: tensor([0, 1, 2, 3, 4])\n",
            "train_mask shape: torch.Size([2277]), sum: 1092\n",
            "val_mask shape: torch.Size([2277]), sum: 729\n",
            "test_mask shape: torch.Size([2277]), sum: 456\n",
            "Epoch: 001, Loss: 1.6155, Val Acc: 0.3416, Test Acc: 0.3026\n",
            "Epoch: 011, Loss: 1.2551, Val Acc: 0.3717, Test Acc: 0.3772\n",
            "Epoch: 021, Loss: 1.0520, Val Acc: 0.3471, Test Acc: 0.3202\n",
            "Epoch: 031, Loss: 0.9330, Val Acc: 0.3498, Test Acc: 0.3377\n",
            "Epoch: 041, Loss: 0.8282, Val Acc: 0.3484, Test Acc: 0.3399\n",
            "Epoch: 051, Loss: 0.7502, Val Acc: 0.3594, Test Acc: 0.3268\n",
            "Epoch: 061, Loss: 0.7481, Val Acc: 0.3498, Test Acc: 0.3202\n",
            "Epoch: 071, Loss: 0.7321, Val Acc: 0.3374, Test Acc: 0.3202\n",
            "Epoch: 081, Loss: 0.6549, Val Acc: 0.3374, Test Acc: 0.3224\n",
            "Epoch: 091, Loss: 0.6892, Val Acc: 0.3265, Test Acc: 0.3311\n",
            "\n",
            "Best Validation Accuracy on Chameleon: 0.3882\n",
            "Test Accuracy at Best Validation: 0.3925\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.38820301783264743, 0.3925438596491228)"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ],
      "source": [
        "train_single_dataset('Chameleon')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vLkUmwi0c6hF"
      },
      "source": [
        "#Train Texas Dataset (Heterophilic)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "A_LhhxThc_6U",
        "outputId": "5f3a7407-e610-415f-9d82-ffafdb4838b6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Training on Texas ---\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading https://raw.githubusercontent.com/graphdml-uiuc-jlu/geom-gcn/master/new_data/texas/out1_node_feature_label.txt\n",
            "Downloading https://raw.githubusercontent.com/graphdml-uiuc-jlu/geom-gcn/master/new_data/texas/out1_graph_edges.txt\n",
            "Downloading https://raw.githubusercontent.com/graphdml-uiuc-jlu/geom-gcn/master/splits/texas_split_0.6_0.2_0.npz\n",
            "Downloading https://raw.githubusercontent.com/graphdml-uiuc-jlu/geom-gcn/master/splits/texas_split_0.6_0.2_1.npz\n",
            "Downloading https://raw.githubusercontent.com/graphdml-uiuc-jlu/geom-gcn/master/splits/texas_split_0.6_0.2_2.npz\n",
            "Downloading https://raw.githubusercontent.com/graphdml-uiuc-jlu/geom-gcn/master/splits/texas_split_0.6_0.2_3.npz\n",
            "Downloading https://raw.githubusercontent.com/graphdml-uiuc-jlu/geom-gcn/master/splits/texas_split_0.6_0.2_4.npz\n",
            "Downloading https://raw.githubusercontent.com/graphdml-uiuc-jlu/geom-gcn/master/splits/texas_split_0.6_0.2_5.npz\n",
            "Downloading https://raw.githubusercontent.com/graphdml-uiuc-jlu/geom-gcn/master/splits/texas_split_0.6_0.2_6.npz\n",
            "Downloading https://raw.githubusercontent.com/graphdml-uiuc-jlu/geom-gcn/master/splits/texas_split_0.6_0.2_7.npz\n",
            "Downloading https://raw.githubusercontent.com/graphdml-uiuc-jlu/geom-gcn/master/splits/texas_split_0.6_0.2_8.npz\n",
            "Downloading https://raw.githubusercontent.com/graphdml-uiuc-jlu/geom-gcn/master/splits/texas_split_0.6_0.2_9.npz\n",
            "Processing...\n",
            "Done!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "data.y shape: torch.Size([183]), unique labels: tensor([0, 1, 2, 3, 4])\n",
            "train_mask shape: torch.Size([183]), sum: 87\n",
            "val_mask shape: torch.Size([183]), sum: 59\n",
            "test_mask shape: torch.Size([183]), sum: 37\n",
            "Epoch: 001, Loss: 1.7321, Val Acc: 0.5254, Test Acc: 0.6486\n",
            "Epoch: 011, Loss: 0.8730, Val Acc: 0.5763, Test Acc: 0.6216\n",
            "Epoch: 021, Loss: 0.7520, Val Acc: 0.5763, Test Acc: 0.5946\n",
            "Epoch: 031, Loss: 0.5076, Val Acc: 0.5593, Test Acc: 0.5676\n",
            "Epoch: 041, Loss: 0.4576, Val Acc: 0.4915, Test Acc: 0.5405\n",
            "Epoch: 051, Loss: 0.4339, Val Acc: 0.5254, Test Acc: 0.4595\n",
            "Epoch: 061, Loss: 0.3919, Val Acc: 0.5085, Test Acc: 0.5135\n",
            "Epoch: 071, Loss: 0.3884, Val Acc: 0.5424, Test Acc: 0.4054\n",
            "Epoch: 081, Loss: 0.3080, Val Acc: 0.4915, Test Acc: 0.3784\n",
            "Epoch: 091, Loss: 0.4096, Val Acc: 0.5085, Test Acc: 0.4865\n",
            "\n",
            "Best Validation Accuracy on Texas: 0.6102\n",
            "Test Accuracy at Best Validation: 0.5946\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.6101694915254238, 0.5945945945945946)"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ],
      "source": [
        "train_single_dataset('Texas')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o2b45GtpdNBL"
      },
      "source": [
        "#Logging Metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "X6TcCvubdOxF",
        "outputId": "3c390811-d428-4bee-81af-a08ce23317c3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Training on Cora ---\n",
            "\n",
            "Training completed in 1.93s for Cora\n",
            "Results saved to Cora_results.csv\n",
            "\n",
            "--- Training on PubMed ---\n",
            "\n",
            "Training completed in 7.74s for PubMed\n",
            "Results saved to PubMed_results.csv\n",
            "\n",
            "--- Training on CiteSeer ---\n",
            "\n",
            "Training completed in 2.87s for CiteSeer\n",
            "Results saved to CiteSeer_results.csv\n",
            "\n",
            "--- Training on Squirrel ---\n",
            "\n",
            "Training completed in 13.55s for Squirrel\n",
            "Results saved to Squirrel_results.csv\n",
            "\n",
            "--- Training on Chameleon ---\n",
            "\n",
            "Training completed in 3.67s for Chameleon\n",
            "Results saved to Chameleon_results.csv\n",
            "\n",
            "--- Training on Texas ---\n",
            "\n",
            "Training completed in 0.68s for Texas\n",
            "Results saved to Texas_results.csv\n",
            "Summary results saved to all_datasets_summary.csv\n"
          ]
        }
      ],
      "source": [
        "import time\n",
        "import pandas as pd\n",
        "\n",
        "# Example results collection during training\n",
        "def train_and_log(dataset_name):\n",
        "    print(f\"\\n--- Training on {dataset_name} ---\\n\")\n",
        "    dataset = load_dataset(dataset_name)\n",
        "    data = dataset[0]\n",
        "\n",
        "    data = fix_masks(data)  # Ensure masks are valid\n",
        "    if len(data.y.shape) > 1:\n",
        "        data.y = data.y.argmax(dim=1)\n",
        "\n",
        "    # Added num_layers argument to GCN constructor\n",
        "    model = GCN(\n",
        "        input_dim=dataset.num_node_features,\n",
        "        hidden_dim=16,\n",
        "        output_dim=dataset.num_classes,\n",
        "        num_layers=2  # Example: Set num_layers to 2\n",
        "    )\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)\n",
        "    criterion = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "    metrics = {\"epoch\": [], \"train_loss\": [], \"val_acc\": [], \"test_acc\": []}\n",
        "    start_time = time.time()\n",
        "\n",
        "    best_val_acc = 0\n",
        "    for epoch in range(100):\n",
        "        train_loss = train_model(model, data, optimizer, criterion)\n",
        "        val_acc = evaluate_model(model, data, data.val_mask)\n",
        "        test_acc = evaluate_model(model, data, data.test_mask)\n",
        "\n",
        "        metrics[\"epoch\"].append(epoch + 1)\n",
        "        metrics[\"train_loss\"].append(train_loss)\n",
        "        metrics[\"val_acc\"].append(val_acc)\n",
        "        metrics[\"test_acc\"].append(test_acc)\n",
        "\n",
        "        if val_acc > best_val_acc:\n",
        "            best_val_acc = val_acc\n",
        "            best_test_acc = test_acc\n",
        "\n",
        "    total_time = time.time() - start_time\n",
        "    print(f\"Training completed in {total_time:.2f}s for {dataset_name}\")\n",
        "    metrics[\"total_time\"] = total_time\n",
        "\n",
        "    # **Compute the number of parameters**\n",
        "    num_params = sum(p.numel() for p in model.parameters())\n",
        "\n",
        "    # Save metrics to a DataFrame\n",
        "    results_df = pd.DataFrame(metrics)\n",
        "    results_df.to_csv(f\"{dataset_name}_results.csv\", index=False)\n",
        "    print(f\"Results saved to {dataset_name}_results.csv\")\n",
        "\n",
        "    # **Return num_params along with other metrics**\n",
        "    return best_val_acc, best_test_acc, total_time, num_params\n",
        "\n",
        "\n",
        "\n",
        "datasets = ['Cora', 'PubMed', 'CiteSeer', 'Squirrel', 'Chameleon', 'Texas']\n",
        "\n",
        "results = []\n",
        "for dataset_name in datasets:\n",
        "    val_acc, test_acc, runtime, num_params = train_and_log(dataset_name)\n",
        "    results.append({\n",
        "        \"dataset\": dataset_name,\n",
        "        \"num_params\": num_params,  # **Add this line**\n",
        "        \"val_acc\": val_acc,\n",
        "        \"test_acc\": test_acc,\n",
        "        \"runtime\": runtime\n",
        "    })\n",
        "\n",
        "# Save all results to a single summary file\n",
        "summary_df = pd.DataFrame(results)\n",
        "summary_df.to_csv(\"all_datasets_summary.csv\", index=False)\n",
        "print(\"Summary results saved to all_datasets_summary.csv\")"
      ]
    }
  ]
}
