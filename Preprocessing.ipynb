{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s2OEpbDke7J4"
      },
      "source": [
        "# Preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8NUJRIjtfbxf"
      },
      "source": [
        "**Preprocessing Functions**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_bThpkfAfj-r"
      },
      "outputs": [],
      "source": [
        "from torch_geometric.utils import add_self_loops, remove_self_loops, degree, get_laplacian, to_scipy_sparse_matrix\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from torch.nn.functional import normalize\n",
        "from torch.nn.functional import normalize\n",
        "from collections import Counter\n",
        "import scipy.sparse as sp\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "def normalize_features(data):\n",
        "    # Ensure node features exist\n",
        "    if data.x is None:\n",
        "        print(\"No node features found; initializing default features.\")\n",
        "        data.x = torch.ones((data.num_nodes, 1), dtype=torch.float)\n",
        "\n",
        "    # Normalize features\n",
        "    data.x = data.x / data.x.sum(dim=1, keepdim=True)\n",
        "    print(f\"[normalize_features] Normalized features. Shape: {data.x.shape}\")\n",
        "    return data\n",
        "\n",
        "\n",
        "def split_dataset(data, train_ratio=0.6, val_ratio=0.2, seed=42, stratify=True):\n",
        "    # Validate split ratios\n",
        "    if train_ratio + val_ratio > 1.0:\n",
        "        raise ValueError(\"Train and validation ratios must sum to 1.0 or less.\")\n",
        "\n",
        "    num_nodes = data.num_nodes\n",
        "\n",
        "    # Check stratification\n",
        "    class_counts = Counter(data.y.numpy())\n",
        "    if stratify and any(count < 2 for count in class_counts.values()):\n",
        "        print(\"Small class detected; switching to random split.\")\n",
        "        stratify_labels = None\n",
        "    else:\n",
        "        stratify_labels = data.y.numpy() if stratify else None\n",
        "\n",
        "    # Train/test split\n",
        "    train_idx, test_idx = train_test_split(\n",
        "        torch.arange(num_nodes).numpy(),\n",
        "        test_size=1 - train_ratio,\n",
        "        stratify=stratify_labels,\n",
        "        random_state=seed,\n",
        "    )\n",
        "\n",
        "    # Validation/test split\n",
        "    val_idx, test_idx = train_test_split(\n",
        "        test_idx,\n",
        "        test_size=(1 - train_ratio - val_ratio) / (1 - train_ratio),\n",
        "        stratify=stratify_labels[test_idx] if stratify_labels is not None else None,\n",
        "        random_state=seed,\n",
        "    )\n",
        "\n",
        "    # split masks\n",
        "    data.train_mask = torch.zeros(num_nodes, dtype=torch.bool)\n",
        "    data.val_mask = torch.zeros(num_nodes, dtype=torch.bool)\n",
        "    data.test_mask = torch.zeros(num_nodes, dtype=torch.bool)\n",
        "\n",
        "    data.train_mask[train_idx] = True\n",
        "    data.val_mask[val_idx] = True\n",
        "    data.test_mask[test_idx] = True\n",
        "\n",
        "    # split sizes\n",
        "    print(f\"Training nodes: {len(train_idx)}, Validation nodes: {len(val_idx)}, Test nodes: {len(test_idx)}, Total nodes: {data.num_nodes}\")\n",
        "\n",
        "    return data\n",
        "\n",
        "\n",
        "def edge_weights(data):\n",
        "\n",
        "    # Calculate node degrees\n",
        "    row, col = data.edge_index\n",
        "    deg = degree(row, data.num_nodes, dtype=torch.float)\n",
        "\n",
        "    # Compute edge weights (inverse degree)\n",
        "    source_degrees = deg[row]\n",
        "    edge_weight = torch.where(source_degrees > 0, 1.0 / source_degrees, torch.zeros_like(source_degrees))\n",
        "\n",
        "    # Row-wise normalization\n",
        "    row_sum = torch.zeros(data.num_nodes, dtype=torch.float).scatter_add_(0, row, edge_weight)\n",
        "    edge_weight = edge_weight / (row_sum[row] + 1e-8)\n",
        "\n",
        "    # Assign edge weights to the graph\n",
        "    data.edge_weight = edge_weight\n",
        "    print(f\"[edge_weights] Calculated edge weights. Total edges: {data.edge_index.size(1)}\")\n",
        "    return data\n",
        "\n",
        "\n",
        "\n",
        "from torch_geometric.utils import add_self_loops, remove_self_loops\n",
        "\n",
        "def self_loops(data):\n",
        "    \"\"\"\n",
        "    Adds self-loops to the graph and ensures proper edge weight initialization.\n",
        "    \"\"\"\n",
        "    # Remove existing self-loops\n",
        "    num_self_loops_before = torch.sum(data.edge_index[0] == data.edge_index[1]).item()\n",
        "    data.edge_index, _ = remove_self_loops(data.edge_index)\n",
        "\n",
        "    # Add self-loops\n",
        "    num_nodes = data.num_nodes\n",
        "    data.edge_index, _ = add_self_loops(data.edge_index, num_nodes=num_nodes)\n",
        "\n",
        "    # Initialize or append self-loop weights\n",
        "    if 'edge_weight' in data and data.edge_weight is not None:\n",
        "        # Append weights for self-loops\n",
        "        self_loop_weights = torch.ones(num_nodes, dtype=data.edge_weight.dtype, device=data.edge_weight.device)\n",
        "        data.edge_weight = torch.cat([data.edge_weight, self_loop_weights])\n",
        "    else:\n",
        "        # Initialize all edge weights if not present\n",
        "        data.edge_weight = torch.ones(data.edge_index.size(1), dtype=torch.float, device=data.edge_index.device)\n",
        "\n",
        "    num_self_loops_after = torch.sum(data.edge_index[0] == data.edge_index[1]).item()\n",
        "    print(f\"[self_loops] Self-loops added. Before: {num_self_loops_before}, After: {num_self_loops_after}\")\n",
        "\n",
        "    return data\n",
        "\n",
        "\n",
        "\n",
        "def node_degree_as_feature(data, normalize=True, method=\"max\"):\n",
        "\n",
        "    # Calculate node degrees\n",
        "    node_degrees = degree(data.edge_index[0], data.num_nodes)\n",
        "\n",
        "    # Normalize degrees if required\n",
        "    if normalize:\n",
        "        if method == \"max\":\n",
        "            max_degree = node_degrees.max() + 1e-8\n",
        "            node_degrees = node_degrees / max_degree\n",
        "        elif method == \"zscore\":\n",
        "            mean_degree = node_degrees.mean()\n",
        "            std_degree = node_degrees.std() + 1e-8\n",
        "            node_degrees = (node_degrees - mean_degree) / std_degree\n",
        "        else:\n",
        "            raise ValueError(\"Normalization method must be 'max' or 'zscore'.\")\n",
        "\n",
        "    # Avoid duplicate concatenation\n",
        "    if data.x.size(1) > 0 and torch.allclose(data.x[:, -1], node_degrees.unsqueeze(1), atol=1e-8):\n",
        "        return data\n",
        "\n",
        "    # Add node degree as a feature\n",
        "    data.x = torch.cat([data.x, node_degrees.unsqueeze(1)], dim=1)\n",
        "    print(f\"[node_degree_as_feature] Added node degree as a feature. Shape: {data.x.shape}\")\n",
        "\n",
        "    return data\n",
        "\n",
        "\n",
        "def positional_encoding(data, num_encodings=None, proportion=0.1):\n",
        "\n",
        "    num_nodes = data.num_nodes\n",
        "\n",
        "    # Dynamic set number of encodings\n",
        "    if num_encodings is None:\n",
        "        if proportion is not None:\n",
        "            num_encodings = max(1, int(num_nodes * proportion))\n",
        "        else:\n",
        "            num_encodings = min(10, num_nodes - 1)\n",
        "\n",
        "\n",
        "    edge_index, edge_weight = get_laplacian(data.edge_index, normalization='sym', num_nodes=data.num_nodes)\n",
        "    adj_matrix = to_scipy_sparse_matrix(edge_index, edge_attr=edge_weight, num_nodes=data.num_nodes)\n",
        "\n",
        "    laplacian = sp.csgraph.laplacian(adj_matrix, normed=True)\n",
        "\n",
        "    eigenvalues, eigenvectors = np.linalg.eigh(laplacian.toarray())\n",
        "\n",
        "    positional_encodings = torch.tensor(eigenvectors[:, 1:num_encodings + 1], dtype=torch.float)\n",
        "\n",
        "    if data.x is not None:\n",
        "        data.x = torch.cat([data.x, positional_encodings], dim=1)\n",
        "    else:\n",
        "        data.x = positional_encodings\n",
        "\n",
        "    print(f\"[positional_encoding] Added {num_encodings} positional encodings. Shape: {data.x.shape}\")\n",
        "    return data\n",
        "\n",
        "\n",
        "\n",
        "def feature_augmentation(data, noise_level=0.01):\n",
        "\n",
        "    if data.x is None:\n",
        "        print(\"No node features found; skipping feature augmentation.\")\n",
        "        return data\n",
        "\n",
        "    noise = torch.randn_like(data.x) * noise_level\n",
        "    data.x += noise\n",
        "    print(f\"[feature_augmentation] Added noise to features. Shape: {data.x.shape}\")\n",
        "\n",
        "    return data\n",
        "\n",
        "\n",
        "def graph_denoising(data, threshold=0.01, preserve_self_loops=True):\n",
        "\n",
        "    # Handle missing edge weights\n",
        "    if 'edge_weight' not in data or data.edge_weight is None:\n",
        "        data.edge_weight = torch.ones(data.edge_index.size(1), dtype=torch.float)\n",
        "\n",
        "    # Handle empty graphs\n",
        "    if data.edge_index.size(1) == 0:\n",
        "        print(\"Graph has no edges; skipping denoising.\")\n",
        "        return data\n",
        "\n",
        "    # Apply threshold and optionally preserve self-loops\n",
        "    if preserve_self_loops:\n",
        "        self_loop_mask = data.edge_index[0] == data.edge_index[1]\n",
        "        mask = (data.edge_weight > threshold) | self_loop_mask\n",
        "    else:\n",
        "        mask = data.edge_weight > threshold\n",
        "\n",
        "    # Filter edges and weights\n",
        "    num_removed = mask.numel() - mask.sum().item()\n",
        "    data.edge_index = data.edge_index[:, mask]\n",
        "    data.edge_weight = data.edge_weight[mask]\n",
        "\n",
        "    print(f\"[graph_denoising] Removed {num_removed} edges. Remaining edges: {data.edge_index.size(1)}\")\n",
        "\n",
        "    return data\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i0Cnvi01gHHj"
      },
      "source": [
        "**Saving Datasets**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lN7W9mVunJoE"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "def save_dataset(data, dataset_name, is_heterophilic):\n",
        "    # Create directory\n",
        "    dataset_dir = os.path.join(root, f\"processed_{dataset_name}\")\n",
        "    os.makedirs(dataset_dir, exist_ok=True)\n",
        "\n",
        "    heterophilic_preprocessing = [\n",
        "        edge_weights,\n",
        "        graph_denoising,\n",
        "        self_loops,\n",
        "        positional_encoding,\n",
        "        normalize_features,\n",
        "        split_dataset,\n",
        "        feature_augmentation,\n",
        "    ]\n",
        "    homophilic_preprocessing = [\n",
        "        self_loops,\n",
        "        edge_weights,\n",
        "        graph_denoising,\n",
        "        node_degree_as_feature,\n",
        "        normalize_features,\n",
        "        split_dataset,\n",
        "    ]\n",
        "\n",
        "    # Preprocessing based on dataset type\n",
        "    new_data = data.clone()\n",
        "    if is_heterophilic:\n",
        "        for func in heterophilic_preprocessing:\n",
        "            new_data = func(new_data)\n",
        "    else:\n",
        "        for func in homophilic_preprocessing:\n",
        "            new_data = func(new_data)\n",
        "\n",
        "    # Save preprocessed data\n",
        "    new_save_path = os.path.join(dataset_dir, f\"new_{dataset_name}.pt\")\n",
        "    torch.save(new_data, new_save_path)\n",
        "    print(f\"Saved preprocessed data: {new_save_path}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6yK83gyzvCHT"
      },
      "source": [
        "# Heterophilic Datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EKCSIbvEjena"
      },
      "source": [
        "**TEXAS**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "zniv_NuAio7I",
        "outputId": "de676f38-d7b6-4496-e3d1-b949925dc893"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading https://raw.githubusercontent.com/graphdml-uiuc-jlu/geom-gcn/master/new_data/texas/out1_node_feature_label.txt\n",
            "Downloading https://raw.githubusercontent.com/graphdml-uiuc-jlu/geom-gcn/master/new_data/texas/out1_graph_edges.txt\n",
            "Downloading https://raw.githubusercontent.com/graphdml-uiuc-jlu/geom-gcn/master/splits/texas_split_0.6_0.2_0.npz\n",
            "Downloading https://raw.githubusercontent.com/graphdml-uiuc-jlu/geom-gcn/master/splits/texas_split_0.6_0.2_1.npz\n",
            "Downloading https://raw.githubusercontent.com/graphdml-uiuc-jlu/geom-gcn/master/splits/texas_split_0.6_0.2_2.npz\n",
            "Downloading https://raw.githubusercontent.com/graphdml-uiuc-jlu/geom-gcn/master/splits/texas_split_0.6_0.2_3.npz\n",
            "Downloading https://raw.githubusercontent.com/graphdml-uiuc-jlu/geom-gcn/master/splits/texas_split_0.6_0.2_4.npz\n",
            "Downloading https://raw.githubusercontent.com/graphdml-uiuc-jlu/geom-gcn/master/splits/texas_split_0.6_0.2_5.npz\n",
            "Downloading https://raw.githubusercontent.com/graphdml-uiuc-jlu/geom-gcn/master/splits/texas_split_0.6_0.2_6.npz\n",
            "Downloading https://raw.githubusercontent.com/graphdml-uiuc-jlu/geom-gcn/master/splits/texas_split_0.6_0.2_7.npz\n",
            "Downloading https://raw.githubusercontent.com/graphdml-uiuc-jlu/geom-gcn/master/splits/texas_split_0.6_0.2_8.npz\n",
            "Downloading https://raw.githubusercontent.com/graphdml-uiuc-jlu/geom-gcn/master/splits/texas_split_0.6_0.2_9.npz\n",
            "Processing...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[edge_weights] Calculated edge weights. Total edges: 325\n",
            "[graph_denoising] Removed 104 edges. Remaining edges: 221\n",
            "[self_loops] Self-loops added. Before: 16, After: 183\n",
            "[positional_encoding] Added 18 positional encodings. Shape: torch.Size([183, 1721])\n",
            "[normalize_features] Normalized features. Shape: torch.Size([183, 1721])\n",
            "Small class detected; switching to random split.\n",
            "Training nodes: 109, Validation nodes: 37, Test nodes: 37, Total nodes: 183\n",
            "[feature_augmentation] Added noise to features. Shape: torch.Size([183, 1721])\n",
            "Saved preprocessed data: /content/dataset/processed_Texas/new_Texas.pt\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Done!\n",
            "/usr/local/lib/python3.10/dist-packages/scipy/sparse/csgraph/_laplacian.py:470: RuntimeWarning: invalid value encountered in sqrt\n",
            "  w = np.where(isolated_node_mask, 1, np.sqrt(w))\n"
          ]
        }
      ],
      "source": [
        "from torch_geometric.datasets import WebKB\n",
        "\n",
        "root = '/content/dataset'\n",
        "texas_dataset = WebKB(root=root, name='Texas')\n",
        "data = texas_dataset[0]\n",
        "\n",
        "save_dataset(data, \"Texas\", is_heterophilic=True)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8p5v6nqxjfyX"
      },
      "source": [
        "**CHAMELEON**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "DMhHHJYeilQI",
        "outputId": "69fb5ddb-56c8-4fce-f21e-b76f836e7a84"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading https://raw.githubusercontent.com/graphdml-uiuc-jlu/geom-gcn/f1fc0d14b3b019c562737240d06ec83b07d16a8f/new_data/chameleon/out1_node_feature_label.txt\n",
            "Downloading https://raw.githubusercontent.com/graphdml-uiuc-jlu/geom-gcn/f1fc0d14b3b019c562737240d06ec83b07d16a8f/new_data/chameleon/out1_graph_edges.txt\n",
            "Downloading https://raw.githubusercontent.com/graphdml-uiuc-jlu/geom-gcn/f1fc0d14b3b019c562737240d06ec83b07d16a8f/splits/chameleon_split_0.6_0.2_0.npz\n",
            "Downloading https://raw.githubusercontent.com/graphdml-uiuc-jlu/geom-gcn/f1fc0d14b3b019c562737240d06ec83b07d16a8f/splits/chameleon_split_0.6_0.2_1.npz\n",
            "Downloading https://raw.githubusercontent.com/graphdml-uiuc-jlu/geom-gcn/f1fc0d14b3b019c562737240d06ec83b07d16a8f/splits/chameleon_split_0.6_0.2_2.npz\n",
            "Downloading https://raw.githubusercontent.com/graphdml-uiuc-jlu/geom-gcn/f1fc0d14b3b019c562737240d06ec83b07d16a8f/splits/chameleon_split_0.6_0.2_3.npz\n",
            "Downloading https://raw.githubusercontent.com/graphdml-uiuc-jlu/geom-gcn/f1fc0d14b3b019c562737240d06ec83b07d16a8f/splits/chameleon_split_0.6_0.2_4.npz\n",
            "Downloading https://raw.githubusercontent.com/graphdml-uiuc-jlu/geom-gcn/f1fc0d14b3b019c562737240d06ec83b07d16a8f/splits/chameleon_split_0.6_0.2_5.npz\n",
            "Downloading https://raw.githubusercontent.com/graphdml-uiuc-jlu/geom-gcn/f1fc0d14b3b019c562737240d06ec83b07d16a8f/splits/chameleon_split_0.6_0.2_6.npz\n",
            "Downloading https://raw.githubusercontent.com/graphdml-uiuc-jlu/geom-gcn/f1fc0d14b3b019c562737240d06ec83b07d16a8f/splits/chameleon_split_0.6_0.2_7.npz\n",
            "Downloading https://raw.githubusercontent.com/graphdml-uiuc-jlu/geom-gcn/f1fc0d14b3b019c562737240d06ec83b07d16a8f/splits/chameleon_split_0.6_0.2_8.npz\n",
            "Downloading https://raw.githubusercontent.com/graphdml-uiuc-jlu/geom-gcn/f1fc0d14b3b019c562737240d06ec83b07d16a8f/splits/chameleon_split_0.6_0.2_9.npz\n",
            "Processing...\n",
            "Done!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[edge_weights] Calculated edge weights. Total edges: 36101\n",
            "[graph_denoising] Removed 0 edges. Remaining edges: 36101\n",
            "[self_loops] Self-loops added. Before: 50, After: 2277\n",
            "[positional_encoding] Added 227 positional encodings. Shape: torch.Size([2277, 2552])\n",
            "[normalize_features] Normalized features. Shape: torch.Size([2277, 2552])\n",
            "Training nodes: 1366, Validation nodes: 455, Test nodes: 456, Total nodes: 2277\n",
            "[feature_augmentation] Added noise to features. Shape: torch.Size([2277, 2552])\n",
            "Saved preprocessed data: /content/dataset/processed_Chameleon/new_Chameleon.pt\n"
          ]
        }
      ],
      "source": [
        "from torch_geometric.datasets import WikipediaNetwork\n",
        "\n",
        "root = '/content/dataset'\n",
        "chameleon_dataset = WikipediaNetwork(root=root, name='chameleon')\n",
        "data = chameleon_dataset[0]\n",
        "\n",
        "save_dataset(data, \"Chameleon\", is_heterophilic=True)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MpQzmIcrvxIJ"
      },
      "source": [
        "**SQUIRREL**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5icVUWoUonnX",
        "outputId": "1660e06c-78a9-440b-d1ff-61e5bdd96861",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading https://raw.githubusercontent.com/graphdml-uiuc-jlu/geom-gcn/f1fc0d14b3b019c562737240d06ec83b07d16a8f/new_data/squirrel/out1_node_feature_label.txt\n",
            "Downloading https://raw.githubusercontent.com/graphdml-uiuc-jlu/geom-gcn/f1fc0d14b3b019c562737240d06ec83b07d16a8f/new_data/squirrel/out1_graph_edges.txt\n",
            "Downloading https://raw.githubusercontent.com/graphdml-uiuc-jlu/geom-gcn/f1fc0d14b3b019c562737240d06ec83b07d16a8f/splits/squirrel_split_0.6_0.2_0.npz\n",
            "Downloading https://raw.githubusercontent.com/graphdml-uiuc-jlu/geom-gcn/f1fc0d14b3b019c562737240d06ec83b07d16a8f/splits/squirrel_split_0.6_0.2_1.npz\n",
            "Downloading https://raw.githubusercontent.com/graphdml-uiuc-jlu/geom-gcn/f1fc0d14b3b019c562737240d06ec83b07d16a8f/splits/squirrel_split_0.6_0.2_2.npz\n",
            "Downloading https://raw.githubusercontent.com/graphdml-uiuc-jlu/geom-gcn/f1fc0d14b3b019c562737240d06ec83b07d16a8f/splits/squirrel_split_0.6_0.2_3.npz\n",
            "Downloading https://raw.githubusercontent.com/graphdml-uiuc-jlu/geom-gcn/f1fc0d14b3b019c562737240d06ec83b07d16a8f/splits/squirrel_split_0.6_0.2_4.npz\n",
            "Downloading https://raw.githubusercontent.com/graphdml-uiuc-jlu/geom-gcn/f1fc0d14b3b019c562737240d06ec83b07d16a8f/splits/squirrel_split_0.6_0.2_5.npz\n",
            "Downloading https://raw.githubusercontent.com/graphdml-uiuc-jlu/geom-gcn/f1fc0d14b3b019c562737240d06ec83b07d16a8f/splits/squirrel_split_0.6_0.2_6.npz\n",
            "Downloading https://raw.githubusercontent.com/graphdml-uiuc-jlu/geom-gcn/f1fc0d14b3b019c562737240d06ec83b07d16a8f/splits/squirrel_split_0.6_0.2_7.npz\n",
            "Downloading https://raw.githubusercontent.com/graphdml-uiuc-jlu/geom-gcn/f1fc0d14b3b019c562737240d06ec83b07d16a8f/splits/squirrel_split_0.6_0.2_8.npz\n",
            "Downloading https://raw.githubusercontent.com/graphdml-uiuc-jlu/geom-gcn/f1fc0d14b3b019c562737240d06ec83b07d16a8f/splits/squirrel_split_0.6_0.2_9.npz\n",
            "Processing...\n",
            "Done!\n",
            "/usr/local/lib/python3.10/dist-packages/scipy/sparse/csgraph/_laplacian.py:470: RuntimeWarning: invalid value encountered in sqrt\n",
            "  w = np.where(isolated_node_mask, 1, np.sqrt(w))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[edge_weights] Calculated edge weights. Total edges: 217073\n",
            "[graph_denoising] Removed 165400 edges. Remaining edges: 51673\n",
            "[self_loops] Self-loops added. Before: 140, After: 5201\n",
            "[positional_encoding] Added 520 positional encodings. Shape: torch.Size([5201, 2609])\n",
            "[normalize_features] Normalized features. Shape: torch.Size([5201, 2609])\n",
            "Training nodes: 3120, Validation nodes: 1040, Test nodes: 1041, Total nodes: 5201\n",
            "[feature_augmentation] Added noise to features. Shape: torch.Size([5201, 2609])\n",
            "Saved preprocessed data: /content/dataset/processed_Squirrel/new_Squirrel.pt\n"
          ]
        }
      ],
      "source": [
        "# Import the dataset\n",
        "from torch_geometric.datasets import WikipediaNetwork\n",
        "\n",
        "# Define root directory and load dataset\n",
        "root = '/content/dataset'\n",
        "squirrel_dataset = WikipediaNetwork(root=root, name='squirrel')\n",
        "data = squirrel_dataset[0] # Access the data object\n",
        "\n",
        "# Saving dataset\n",
        "save_dataset(data, \"Squirrel\", is_heterophilic=True)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qHbJV2v3mIay"
      },
      "source": [
        "# Homophilic Datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "glZu-Aqgjdsk"
      },
      "source": [
        "**CORA**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "AQMWYLEOi2xl",
        "outputId": "799659d9-39b3-485d-a94e-dce961d38f35"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.x\n",
            "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.tx\n",
            "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.allx\n",
            "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.y\n",
            "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.ty\n",
            "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.ally\n",
            "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.graph\n",
            "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.test.index\n",
            "Processing...\n",
            "Done!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[self_loops] Self-loops added. Before: 0, After: 2708\n",
            "[edge_weights] Calculated edge weights. Total edges: 13264\n",
            "[graph_denoising] Removed 168 edges. Remaining edges: 13096\n",
            "[node_degree_as_feature] Added node degree as a feature. Shape: torch.Size([2708, 1434])\n",
            "[normalize_features] Normalized features. Shape: torch.Size([2708, 1434])\n",
            "Training nodes: 1624, Validation nodes: 542, Test nodes: 542, Total nodes: 2708\n",
            "Saved preprocessed data: /content/dataset/processed_Cora/new_Cora.pt\n"
          ]
        }
      ],
      "source": [
        "from torch_geometric.datasets import Planetoid\n",
        "\n",
        "root = '/content/dataset'\n",
        "cora_dataset = Planetoid(root=root, name='Cora')\n",
        "data = cora_dataset[0]\n",
        "\n",
        "save_dataset(data, \"Cora\", is_heterophilic=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A1HVAAZIjcbG"
      },
      "source": [
        "**CITESEER**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "cTkR-vbzjTWh",
        "outputId": "268f0ea3-58f3-43f4-9346-eb310b8749b2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.citeseer.x\n",
            "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.citeseer.tx\n",
            "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.citeseer.allx\n",
            "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.citeseer.y\n",
            "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.citeseer.ty\n",
            "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.citeseer.ally\n",
            "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.citeseer.graph\n",
            "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.citeseer.test.index\n",
            "Processing...\n",
            "Done!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[self_loops] Self-loops added. Before: 0, After: 3327\n",
            "[edge_weights] Calculated edge weights. Total edges: 12431\n",
            "[graph_denoising] Removed 0 edges. Remaining edges: 12431\n",
            "[node_degree_as_feature] Added node degree as a feature. Shape: torch.Size([3327, 3704])\n",
            "[normalize_features] Normalized features. Shape: torch.Size([3327, 3704])\n",
            "Training nodes: 1996, Validation nodes: 665, Test nodes: 666, Total nodes: 3327\n",
            "Saved preprocessed data: /content/dataset/processed_Citeseer/new_Citeseer.pt\n"
          ]
        }
      ],
      "source": [
        "from torch_geometric.datasets import Planetoid\n",
        "\n",
        "root = '/content/dataset'\n",
        "citeseer_dataset = Planetoid(root=root, name='Citeseer')\n",
        "data = citeseer_dataset[0]\n",
        "\n",
        "save_dataset(data, \"Citeseer\", is_heterophilic=False)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cKAlCMM6jZOF"
      },
      "source": [
        "**PUBMED**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "uT7uNBJNjVBZ",
        "outputId": "c23cf3ea-70e8-4360-868c-dc94b74890d5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.pubmed.x\n",
            "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.pubmed.tx\n",
            "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.pubmed.allx\n",
            "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.pubmed.y\n",
            "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.pubmed.ty\n",
            "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.pubmed.ally\n",
            "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.pubmed.graph\n",
            "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.pubmed.test.index\n",
            "Processing...\n",
            "Done!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[self_loops] Self-loops added. Before: 0, After: 19717\n",
            "[edge_weights] Calculated edge weights. Total edges: 108365\n",
            "[graph_denoising] Removed 832 edges. Remaining edges: 107533\n",
            "[node_degree_as_feature] Added node degree as a feature. Shape: torch.Size([19717, 501])\n",
            "[normalize_features] Normalized features. Shape: torch.Size([19717, 501])\n",
            "Training nodes: 11830, Validation nodes: 3943, Test nodes: 3944, Total nodes: 19717\n",
            "Saved preprocessed data: /content/dataset/processed_PubMed/new_PubMed.pt\n"
          ]
        }
      ],
      "source": [
        "from torch_geometric.datasets import Planetoid\n",
        "\n",
        "root = '/content/dataset'\n",
        "pubmed_dataset = Planetoid(root=root, name='PubMed')\n",
        "data = pubmed_dataset[0]\n",
        "\n",
        "save_dataset(data, \"PubMed\", is_heterophilic=False)\n"
      ]
    }
  ]
}
